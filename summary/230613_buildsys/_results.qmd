# Results 

::: {#fig-exp-results layout-nrow=4 layout-ncol=3}

![](figs/f1.png){#fig-f1}

![](figs/hits_guesses_ratio.png){#fig-hg}

![](figs/guesses_actions_ratio.png){#fig-ga}

![](figs/opening_acc.png){#fig-oa}

![](figs/unbounded_opening_acc.png){#fig-uoa}

![](figs/true_time.png){#fig-tt}

Comparison across experiments and metrics 
:::

In @fig-exp-results, comparisons of ST to the performance of the SVM across the metrics and datasets are displayed. While ST represents one model, each of the SVM models shown in each of figures represents a model that performed unsupervised classification using a different combination of features. The SVM models are organized according to performance based on metric, and only the top three models are shown. 

In @fig-f1, we see that the ST performs better than the best-performing SVMs on experiments A and B, but has considerably worse performance on experiment C. The range of F1-scores displayed in @fig-f1 are comparable to those shown in (CITE drdr). The low values across all models for experiments B and C can be attributed to the rather unbalanced nature of the datasets, which a macro average F1-score penalizes.  

ST has less accuracy in detecting the ratios of hits to guesses in experiments B and C compared to the best-performing SVMs, as shown in @fig-hg. However, as @fig-ga highlights, the SVMs take far more guesses per recorded action of window state change, while ST takes on the order of 1 guess per recorded action. This suggests that the SVMs' superior performance on the hits over guesses metric is due to probability, rather than an embedding of the underlying physical dynamics. (FIX: currently guesses to action is actually showing worst perfroming SVMs, to make this analysis valid need to sort SVM based on hits to guess ration) 

@fig-oa and @fig-uoa display the performance on the opening and unbounded opening accuracy metrics. We observe that ST has a far higher performance for experiment A, a slightly better performance for experiment C, and a score of 0 for experiment B. The unbounded opening accuracy allows us to compare just how "off" models are. We see that ST performs "less poorly" than the SVM across all experiments. This suggests that ST predictions of when window states change are, on average, closer in time to reality than the predictions of the SVM models. 

The true time and false time metrics, shown in @fig-tt operates as an analog to a weighted average F1-score, in that the the imbalance between window open and close is inherently taken into account. ST is true more often and false less often (false opening time graph not shown) for experiments A and B. The reverse is true for experiment C. 

We see that the ST performs poorly on experiment C, which is the most unfavorable dataset since it had unbalanced window states and narrow distinction between indoor and outdoor temperature.
Despite poor performance on this particularly unfavroable dataset, the ST shows comparable performance to the SVMs across all metrics for the other two datasets. This finding is significant, given that the SVMs shown in the graphs have made unsupervised classification decisions a wide range of combinations of features, and then sorted based on performance to reveal the optimal feature set for a particular experiment. ST method thus shows potential as a powerful teqhnique with robust performance across both favorable and unfavorable datasets.
