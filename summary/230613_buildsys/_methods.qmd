# Methods

TODO brief overview here...

## Data Collection 

We collected data for short periods of time over three months in the summer. We placed HOBO® Temp-RH 2.5% Data Logger (UX100-011) (CITE) sensors in two adjacent rooms in a multi-family residential building, and measured indoor temperatures, $T_{meas}(t)$ and relative humidity, $RH_{meas}(t)$. We also collected data on the ambient temperature, $T_{amb}(t)$ and relative humidity $RH_{amb}(t)$ from a local weather station (CITE). In one room, the window state, $W(t)$, was held constant. In the adjacent room, window state was allowed to vary between open and closed. 

Descriptions of the data in the rooms where the window state varied are provided in @tbl-data-collected. The experiments are labeled as experiments A, B, and C, corresponding to data recorded at the beginning on July 20, July 27, and September 8 respectively. Experiment B was much longer than the other experiments, spanning 14 days. Data for experiments A and C were collected for less than 4 days. The window was open for 5 percent of time recorded time in experiment B, which is shorter than the the 20 percent or greater recorded for other experiments (FIX).

```{python}
#| label: tbl-data-collected
#| tbl-cap: Data Collected
#| echo: false

import pandas as pd
from IPython.display import Markdown
from tabulate import tabulate

tdf = pd.read_pickle("variables/measurements_df.pkl")
tdf_md = tdf.to_markdown(floatfmt=".4f")
Markdown(tdf_md)

```

To develop an understanding of how the indoor and ambient quantities of interest changed across the different experiments, we examined the distributions of the data. The internal relative humidity was higher than the ambient relative humidity across experiments. The temperature distributions, which had distinctive fluctiations across experiments, are shown in @fig-data-collected. We observe that experiment B had the widest distribution of indoor temperatures. Experiment C had internal temperatures that were higher and closer to the mean of the ambient temperatures. This can be attributed to higher ambient temperatures that were recorded during this time period, which makes for a particularly interesting edge case. 

::: {#fig-data-collected layout-ncol=3}

![](figs/dist_expa.png){#fig-dexpa}

![](figs/dist_expb.png){#fig-dexpb}

![](figs/dist_expc.png){#fig-dexpc}

Distributions of temperature across experiments
:::

In summary, the three receorded datasets present various degrees of difficulty for a detection algorithm. Experiment A presents the most favorable case for window state detection, as it has a fairly even amount of opening and closed states, as well as distinct indoor and ambient measured temperatures and relative humidities. Although experiment B also has noteable distinctions in terms of temperature and relative humidity, the window states are extremely unbalanced. Experiment C has both unbalanced window states and little distincition between indoor and outdoor temperature. 



## Detection Methods

In the following section, we discuss the two methods that were used to detect window state changes. The first is a smoothing technique that we have developed, and the second is a machine learning method that has been studied in the literature (CITE).

### Smoothing Technique

Our approach relies on an intuitive understanding of window operation detection. We expect a typical time series recording of a quantity of interest, in this case the measured indoor temperature, $T_{meas}(t)$, to contain information that reflects the seasonality of ambient quantities, noise due to occurrences within the space where measurements are being taken, and the desired signal of changes in window state. The way in which these three components of a measurement are combined is unknown, and, the noise component in particular cannot be known based on the measurements we have collected. As we have recorded information about the ambient temperature, $T_{amb}(t)$, we focus on removing the seasonality from $T_{meas}(t)$, hypothesizing that this will reveal the  window state signal and noise. An optimal technique will additinally isolate the window state signal from the unknown noise. 

To remove the seasonality from the measured data, we examined three methods of de-seasonalizing: using sinusoidal fit, using an exponentially weighted mean function, and using a seasonal-trend-decomposition (CITE all), which consists of optimizing the parameters of a sine function to fit a specific time series, purely reflects seasonality. However, this does not nessecarily account for the seasonality that  $T_{meas}(t)$ experiences due to changes in outdoor temperature. The exponentially weighted mean function is simply a moving average of the indoor temperature signal – it captures some seasnality and some noise. The seasonal component of a seasonal trend decomposition represents a middle ground. In preliminary studies, we found that the exponentially weighted mean function performed the best in isolating window detection, which follows from our intuition that both the seasonal components and noise need to be isolated.

The smoothing technique proceeds as follows. The goal is to identify $W(t)$, window state as a function of time. This can take on two values: 0, representing window closed, or 1, representing window open. We have an observed variable $T(t) = T_{meas}(t)$, which represents the measurement of the indoor temperature. We apply an exponentially weighted mean (EWM) function to $T(t)$, creating a smoothed time series, $\overline{T(t)}$, which ideally removes strong peaks that would reflect changes in window state, and isolates information concerning the seasonal response and additional noise. This technique operates under the assumption that the instantaneous change in indoor temperature due to window opening is greater than any other potential source of instantaneous temperature change. In reality, other unknown occurences within a room might cause large temperature spikes, which would interfere with the efficacy of $\overline{T(t)}$. Subtracting $\overline{T(t)}$ from $T(t)$ yields $T'(t)$, which is a time series that reflects changes in the window state and some additional noise. 

In order to confidently identify the where the changes in window state occur, we examine the first and second derivatives of $T'(t)$, $\frac{dT'(t)}{dt}$ and $\frac{d^2T'(t)}{dt^2}$. The second derivative  is particularly effective for identifying change points. In order to predict where window changes occur, we apply a principles from statistical hypothesis testing. We assume that the time series  $\frac{d^2T'(t)}{dt^2}$ is normally distributed. Therefore, any value in  $\frac{d^2T'(t)}{dt^2}$ that is more than 2 standard deviations away from the mean of this time series is unlikely to occur, and could possibly indicate an instance of a change in window state.  We will use these unlikely values as initial guesses $G(t)$. They take on positive or negative values depending on whether they are predicting a transition from window open to close, or window close to open. Therefore, we round the values of $G(t)$ to 0 or 1 to reflect this. Finally, we interpolate between the rounded values of  $G(t)$, so that we only predict a change in window state when $G(t)$ transitions between 0 and 1. This prediction of the window state is called $I(t)$. 

### Machine Learning Method: SVM
We used a separate method in order to compare the efficacy of the smoothing technique that we developed. We chose to use an SVM, as it has been shown in literature (CITE de Rautlin de Roy) to have robust performance across a wide range of features. SVMs are straightforward to implement with relatively few hyper-parameters, as compared to other methods that have been recently shown to have high performance on the window detection problem. For our particular interest in unsupervised problems, the availability of an unsupervied implementation of a SVM through sklearn's One-Class SVM is also ideal (CITE).

In a similar way to a linear regression, SVMs approximate a line of best fit using the features and labels provided. However, with the use of kernel functions that can map high dimensional feature spaces to ones of lower dimensions, SVMs also effectively create hyperplanes of best fit in order to classify datasets. The use of a kernel function also enables the introduction of mappings that may be a better suited to the structure of features in a given dataset (CITE).

Our focus was trying to get the best performance from the SVM using an optimal set of features with optimal pre-processing functions applied. We therefore came up with combinations of the various features we had access to, as well as their derivatives and differences from one another. The base features were: ambient temperature $T_{amb}(t)$, measured temperature $T_{meas}(t)$, and their derivatives, $\frac{\mathrm{d}}{\mathrm{d}t}T_{meas}$ and $\frac{\mathrm{d}}{\mathrm{d}t}T_{amb}$. We also considered the difference between ambient temperature and measured temperature $T_{amb} - T_{meas}$ , the difference between measured temperature and the derivative of measured temperature, $T_{meas} - \frac{\mathrm{d}}{\mathrm{d}t}T_{meas}$. We also had the same features for relative humidity. After creating combinations of these base features, we had a test set of 113 combinations.

For each experiment we recorded data for, we created an SVM for each of the combinations in the test set, which resulted in 3x113 different SVM models. We chose not to perform hyperparameter tuning for the SVMs, and used the default parameters defined by sklearn which consists of a radial basis function kernel. As mentioned above, we are interested in the developing an unsupervised detection method. Therefore, we used the One-Class SVM implementation which is ideal for anomally detection. Outliers are identified by clustering features to create the hyperplanes for classification. 

The SVM approach differs from the smoothing technique that we have developed, and is similar to other window techniques in that it is a highly generalizable method that is not developed with window detection in mind. A wide array of input features can be tested in order to get an acceptable prediction accuracy. However, this might not be acceptable in practice, as different scenarios or window operation behaviors,  will demand different sets of input features. In the event the true window detection pattern is unknown, it will be difficult to know which input features are giving a trustworthy result. 

## Evaluation Metrics

We used 3 sets of metics to evaluate the methods we chose. The first two sets follow from a recent paper that compared the efficacy of different machine learning algorithms for window detection(CITE) , and the final set of metrics were developed to more closely examine specific behavior of chosen models. 

### Standard Metrics

**Macro average F-1 score**: The F-1 score is a classic metric for evaluating the performance of classification algorithms. It it condenses information about the precision of a model in predicting a certain class, as well as its ability to recall the the available data. The macro averaged F-1 score averages the F-1 scores for all individual classes, but does not introduce weights in the averaging to reflect that the classes may be unbalanced. As shown in @tbl-data-collected, the data sets we have very from fairly balanced in experiment A, to highly unbalanced in experiment B. Therefore, using the macro average F-1 score provides a worst case performance. Using this F1-score also provides a basis of comparison to (CITE) . The implementation for computing this metric comes from scikit learns classification report (CITE).

### De Rautlin de Roy Metrics

The following metrics were introduced by (CITE), and represent window detection specific evaluation metrics. 

**Opening Accuracy**: The opening accuracy is an average of scores given to each opening instance predicted by a window detection model. An opening instance is an interval on $t$, such as $t_{nk} =  t_n,  t_{n+1}, \dots , t_k$, where the final guess from the algorithm, $I(t_{nk}) = 1$. An opening receives a score of 0 if it does not correspond to a a true opening: $I(t_{nk}) = 1, W(t_{nk} = 0)$, and a score of 1 it perfectly corresponds to a true opening: $I(t_{nk}) = 1, W(t_{nk} = 1)$. The score for a given predicted opening interval decreases by a penalty of 0.33 for each time step that does not align to a true opening. Therefore, a predicted opening interval $I(t_{nk}) = 1$ that aligns a true opening interval will have a score of at most 0 if it extends for more than two times steps away from a true value. For the opening accuracy metric desciribed by (CITE drdr), the value for each opening interval are bounded at 0, and then are averaged to get the opening accuracy for an entire model. Here we also consider an unbounded opening accuracy where the score for a given interval is not bounded at 0 but is allowed to become negative. This unbounded metric provides clearer indication of  how "off" models are, given that due to the unfavorable nature of some of the datasets collected, many models we examined actually had a bounded opening accuracy of 0.

**True or False Opening Time**: This true opening time reflects the total amount of time when $I(t) = W(t) = 1$. False opening times occur when $I(t) = 1, W(t) = 0$. The metric is somewhat similar to the F-1 score in that rather than looking at specifically at change points, it provides information the entire time period. 

### Custom Metrics 

We developed the final set of metrics based on the intuition that if a model is perfectly able to capture the specific times when a window state changes, then it is performing extremely well. We classify a "guess", as any value of $I(t)$, and an "action" as any value of $W(t)$. A "hit" occurs when  $I(t) = W(t)$. Like (CITE) we consider a prediction accurate if it is withing at most 2 timestamps of the true occurrence. Therefore, a "near hit" occurs when $I(t) = W(t \pm 2)$. The metrics we examine are given below. 

**Hits + Near Hits Over Guesses**: This metric accounts for the variability in guesses. A ratio of 1 indicates that all guesses taken by the model were accurate within two timestamps. A ratio close to 0 indicates that a lot of guesses were taken, but relatively few were close to where change occurred in the true window state.

**Guesses Over Actions**: This metric implicitly accounts for the ability of the model to capture the pattern of the changes in window state. If the true window state changed only 10 times, but the model predicts changes on the order of 100, then the model is performing poorly. A perfect score is 1. 


